{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46585d0",
   "metadata": {},
   "source": [
    "# AAM-IPL Week-3 — Boston Housing Price Prediction (Corrected)\n",
    "**Converted notebook — cleaned and made robust**  \n",
    "- Author: K V Amarnath Reddy  \n",
    "- Roll No.: 229X1A33B2  \n",
    "- Branch: CSM — Emerging Technologies in Computer Science  \n",
    "- Date converted: 2025-11-13  \n",
    "\n",
    "**What changed / improvements:**  \n",
    "- Robust target detection (`MEDV`, `medv`, `target`) or fallback to the last column.  \n",
    "- Uses `mean_absolute_percentage_error` when available, otherwise safe fallback.  \n",
    "- Uses matplotlib for all plots (no seaborn).  \n",
    "- Provides a small synthetic fallback dataset if the CSV is missing so the notebook cells can be executed for testing.\n",
    "- Clear save/load checks and friendly messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f986cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements (optional)\n",
    "# If you run into missing packages, install them in your environment, for example:\n",
    "# pip install scikit-learn matplotlib pandas joblib\n",
    "\n",
    "print('This cell lists recommended packages. Install them only if missing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54018010",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "try:\n",
    "    # newer sklearn versions include this\n",
    "    from sklearn.metrics import mean_absolute_percentage_error\n",
    "    _mape_available = True\n",
    "except Exception:\n",
    "    _mape_available = False\n",
    "\n",
    "print('Libraries loaded. mean_absolute_percentage_error available:', _mape_available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a76efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset (search for common filenames in current directory)\n",
    "possible_filenames = [\n",
    "    \"BostonHousing - BostonHousing.csv\",\n",
    "    \"BostonHousing.csv\",\n",
    "    \"boston_housing.csv\",\n",
    "    \"BostonHousing - BostonHousing Data.csv\",\n",
    "    \"Boston.csv\"\n",
    "]\n",
    "\n",
    "csv_path = None\n",
    "for fn in possible_filenames:\n",
    "    if os.path.exists(fn):\n",
    "        csv_path = fn\n",
    "        break\n",
    "\n",
    "if csv_path is None:\n",
    "    print('No CSV found in the notebook directory.')\n",
    "    print('If you want to use the original dataset, place one of these filenames in the same folder as the notebook:')\n",
    "    print('\\n'.join(possible_filenames))\n",
    "    print('\\nProceeding with a small synthetic sample dataset so you can run the notebook immediately.')\n",
    "    # create a small synthetic dataset similar in shape to Boston (13 features + target)\n",
    "    np.random.seed(0)\n",
    "    X_synth = np.random.randn(50, 13)\n",
    "    coef = np.random.randn(13)\n",
    "    y_synth = X_synth.dot(coef) + np.random.randn(50) * 0.5 + 22.0\n",
    "    columns = [f'feature_{i+1}' for i in range(X_synth.shape[1])] + ['MEDV']\n",
    "    df = pd.DataFrame(np.hstack([X_synth, y_synth.reshape(-1,1)]), columns=columns)\n",
    "    print('Synthetic sample created with shape', df.shape)\n",
    "else:\n",
    "    print('Loading dataset from:', csv_path)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print('Dataset shape:', df.shape)\n",
    "\n",
    "# show top rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ffc549",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Detect target column\n",
    "def detect_target_column(df):\n",
    "    candidates = ['MEDV', 'medv', 'medians', 'target', 'PRICE', 'price']\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    # fallback to last column\n",
    "    return df.columns[-1]\n",
    "\n",
    "target_col = detect_target_column(df)\n",
    "print('Detected target column:', target_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9475768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Watermark helper (optional). Place 'AAM-IPL-Header-6.png' in the notebook folder to use.\n",
    "def add_watermark(ax, logo_path='AAM-IPL-Header-6.png', alpha=0.25, zoom=0.6):\n",
    "    try:\n",
    "        if os.path.exists(logo_path):\n",
    "            logo = mpimg.imread(logo_path)\n",
    "            imagebox = OffsetImage(logo, zoom=zoom)\n",
    "            ab = AnnotationBbox(imagebox, (0.5, 0.5), frameon=False, xycoords='axes fraction', box_alignment=(0.5,0.5), pad=0)\n",
    "            ax.add_artist(ab)\n",
    "    except Exception as e:\n",
    "        print('Watermark error:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb5fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic EDA: summary statistics and histograms for features\n",
    "print('Dataframe shape:', df.shape)\n",
    "print('\\nSummary statistics:')\n",
    "display(df.describe())\n",
    "\n",
    "# Histograms for all numeric columns (small sample sets will still show useful plots)\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "figsize = (8, 4)\n",
    "for col in num_cols:\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.hist(df[col].dropna(), bins=30)\n",
    "    ax.set_title(f'Histogram: {col}')\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Frequency')\n",
    "    try:\n",
    "        add_watermark(ax)\n",
    "    except:\n",
    "        pass\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1aeb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Correlation matrix heatmap constructed using matplotlib (no seaborn)\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "corr = df[num_cols].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "cax = ax.matshow(corr, cmap='coolwarm')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticks(range(len(num_cols)))\n",
    "ax.set_yticks(range(len(num_cols)))\n",
    "ax.set_xticklabels(num_cols, rotation=90)\n",
    "ax.set_yticklabels(num_cols)\n",
    "ax.set_title('Correlation matrix (matplotlib)')\n",
    "try:\n",
    "    add_watermark(ax)\n",
    "except:\n",
    "    pass\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f505e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess: prepare X and y, scale features, and split\n",
    "target = target_col\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target].astype(float)\n",
    "\n",
    "print('Feature matrix shape:', X.shape)\n",
    "print('Target vector shape:', y.shape)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "print('Train size:', X_train.shape[0], 'Test size:', X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Linear Regression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print('Model trained.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a0dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation metrics: R2, MSE, RMSE, MAE, MAPE (safe)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "if _mape_available:\n",
    "    try:\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "    except Exception:\n",
    "        mape = np.nan\n",
    "else:\n",
    "    # safe manual MAPE: ignore zero targets to avoid division by zero\n",
    "    y_test_arr = np.array(y_test, dtype=float)\n",
    "    y_pred_arr = np.array(y_pred, dtype=float)\n",
    "    nonzero_mask = y_test_arr != 0\n",
    "    if nonzero_mask.sum() == 0:\n",
    "        mape = np.nan\n",
    "    else:\n",
    "        mape = (np.abs((y_test_arr[nonzero_mask] - y_pred_arr[nonzero_mask]) / y_test_arr[nonzero_mask]).mean()) * 100\n",
    "\n",
    "print(f'R^2: {r2:.4f}')\n",
    "print(f'MSE: {mse:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "print(f'MAE: {mae:.4f}')\n",
    "print(f'MAPE: {mape:.2f}%' if not np.isnan(mape) else 'MAPE: NaN (could not compute)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb5d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot actual vs predicted (sorted by index for visual clarity)\n",
    "import pandas as pd\n",
    "y_test_series = pd.Series(y_test).reset_index(drop=True)\n",
    "y_pred_series = pd.Series(y_pred).reset_index(drop=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(y_test_series.values, label='Actual', linewidth=2)\n",
    "ax.plot(y_pred_series.values, label='Predicted', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('Sample index (sorted by original order in test set)')\n",
    "ax.set_ylabel(target_col)\n",
    "ax.set_title('Actual vs Predicted')\n",
    "ax.legend()\n",
    "try:\n",
    "    add_watermark(ax)\n",
    "except:\n",
    "    pass\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36cbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show coefficients and intercept\n",
    "feature_names = X.columns.tolist()\n",
    "coeffs = pd.DataFrame({'feature': feature_names, 'coefficient': model.coef_})\n",
    "coeffs = coeffs.sort_values(by='coefficient', key=lambda s: s.abs(), ascending=False)\n",
    "display(coeffs.reset_index(drop=True))\n",
    "print('Intercept:', float(model.intercept_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f630d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save model and scaler for later use\n",
    "import joblib\n",
    "joblib.dump(model, 'linear_regression_model.joblib')\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "print('Saved model to linear_regression_model.joblib and scaler to scaler.joblib')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
